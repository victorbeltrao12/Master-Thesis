{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Librarys:\n",
        "Python - 3.11.7\n",
        "Numba - 0.58.1\n",
        "Pyarrow - 14.0.0\n",
        "Pandas - 1.5.3\n",
        "Numpy - 1.26.3\n",
        "Pywt - 1.5.0\n",
        "Aeon - 0.7.0\n",
        "Scipy - 1.11.4\n",
        "Matplotlib - 3.8.0\n",
        "Sklearn - 1.2.2\n",
        "Tqdm - 4.66.1\n",
        "tsfresh - 0.20.1\n",
        "tslearn - 0.6.3\n",
        "Feature-engine - 1.7\n",
        "\n",
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install Matplotlib\n",
        "%pip install Sklearn\n",
        "%pip install pywavelets\n",
        "%pip install Numba\n",
        "%pip install Pyarrow\n",
        "%pip install Pandas\n",
        "%pip install Numpy\n",
        "%pip install Scipy\n",
        "%pip install Tqdm\n",
        "%pip install feature-engine\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier, DrCIFClassifier\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from aeon.classification.base import BaseClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "\n",
        "from feature_engine.encoding import LabelEncoder\n",
        "\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Função para carregar os dados do repositório UCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Função das diferentes representações de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_data_math(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "    \n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    \n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "\n",
        "    # Original Data\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "\n",
        "    # FFT Data\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "\n",
        "    # DWT Data\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "\n",
        "    # Adding IDs to DataFrames\n",
        "    df_X['id'] = df_FFT['id'] = df_DWT['id'] = df_PAA['id'] = df_SAX['id'] = range(len(df_X))\n",
        "    \n",
        "    # Merging all DataFrames on 'id'\n",
        "    final_df = df_X.merge(df_FFT, on='id', suffixes=('_X', '_FFT'))\n",
        "    final_df = final_df.merge(df_DWT, on='id', suffixes=('', '_DWT'))\n",
        "    final_df = final_df.merge(df_PAA, on='id', suffixes=('', '_PAA'))\n",
        "    final_df = final_df.merge(df_SAX, on='id', suffixes=('', '_SAX'))\n",
        "    \n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treino & Predição"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedMetaClassifier:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.param_grids = [\n",
        "            {\n",
        "                \"C\":[0.01, 0.1, 1, 10, 100, 1000],\n",
        "                \"kernel\":['linear','rbf','poly','sigmoid'],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.clf1 = SVC(probability=True)\n",
        "        self.clf2 = SVC(probability=True)\n",
        "        self.clf3 = SVC(probability=True)\n",
        "        self.clf4 = SVC(probability=True)\n",
        "        self.meta_clf = RidgeClassifierCV(np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Treinar classificadores base com GridSearchCV\n",
        "        best_classifiers = []\n",
        "        for clf, param_grid in zip(self.classifiers, self.param_grids):\n",
        "            grid_search = GridSearchCV(clf, param_grid, cv=2, n_jobs=2)\n",
        "            grid_search.fit(X, y)\n",
        "            best_classifiers.append(grid_search.best_estimator_)\n",
        "            print(f'Best parameters for {clf}: {grid_search.best_params_}')\n",
        "\n",
        "        self.classifiers = best_classifiers\n",
        "\n",
        "        # Obter probabilidades dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, len(np.unique(y))))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Treinar meta-classificador\n",
        "        self.meta_clf.fit(meta_features, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, clf.n_classes_))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Previsão final usando o meta-classificador\n",
        "        return self.meta_clf.predict(meta_features)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, clf.n_classes_))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Previsão probabilística final usando o meta-classificador\n",
        "        return self.meta_clf.predict_proba(meta_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_data = []\n",
        "for dataset_name in univariate_equal_length:\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    \n",
        "    model_classifier = CombinedMetaClassifier()\n",
        "    model_classifier.fit(features_train, target_train)\n",
        "    y_hat = model_classifier.predict(features_test)\n",
        "    accuracy = accuracy_score(target_test, y_hat)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df_extf = pd.DataFrame(accuracy_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
