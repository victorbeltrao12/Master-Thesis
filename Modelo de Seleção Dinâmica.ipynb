{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Librarys:\n",
        "Python - 3.11.7\n",
        "Numba - 0.58.1\n",
        "Pyarrow - 14.0.0\n",
        "Pandas - 1.5.3\n",
        "Numpy - 1.26.3\n",
        "Pywt - 1.5.0\n",
        "Aeon - 0.7.0\n",
        "Scipy - 1.11.4\n",
        "Matplotlib - 3.8.0\n",
        "Sklearn - 1.2.2\n",
        "Tqdm - 4.66.1\n",
        "tsfresh - 0.20.1\n",
        "tslearn - 0.6.3\n",
        "Feature-engine - 1.7\n",
        "\n",
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install Matplotlib\n",
        "%pip install Sklearn\n",
        "%pip install pywavelets\n",
        "%pip install Numba\n",
        "%pip install Pyarrow\n",
        "%pip install Pandas\n",
        "%pip install Numpy\n",
        "%pip install Scipy\n",
        "%pip install Tqdm\n",
        "%pip install feature-engine\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier, DrCIFClassifier\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from aeon.classification.base import BaseClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "\n",
        "from feature_engine.encoding import LabelEncoder\n",
        "\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Função para carregar os dados do repositório UCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Função das diferentes representações de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_data_math(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "    \n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    \n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "\n",
        "    # Original Data\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "\n",
        "    # FFT Data\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "\n",
        "    # DWT Data\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "\n",
        "    # Adding IDs to DataFrames\n",
        "    df_X['id'] = df_FFT['id'] = df_DWT['id'] = df_PAA['id'] = df_SAX['id'] = range(len(df_X))\n",
        "    \n",
        "    # Merging all DataFrames on 'id'\n",
        "    final_df = df_X.merge(df_FFT, on='id', suffixes=('_X', '_FFT'))\n",
        "    final_df = final_df.merge(df_DWT, on='id', suffixes=('', '_DWT'))\n",
        "    final_df = final_df.merge(df_PAA, on='id', suffixes=('', '_PAA'))\n",
        "    final_df = final_df.merge(df_SAX, on='id', suffixes=('', '_SAX'))\n",
        "    \n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### Modelo de Seleção dinâmica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.meta_clf = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "        train_preds = [clf.predict(X) for clf in self.classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treino & Predição"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_data = []\n",
        "for dataset_name in univariate_equal_length:\n",
        "    X_train, X_test, target_train, target_test = load_data(dataset_name)\n",
        "    \n",
        "    #Training\n",
        "    best_wl = choose_wavelet(X_train)\n",
        "    features_train = transform_series(X_train, best_wl)\n",
        "    \n",
        "    #Testing\n",
        "    features_test = transform_series(X_test, best_wl)\n",
        "    \n",
        "    feature_extractor = CombinedDecisionForest()\n",
        "    feature_extractor.fit(features_train, target_train)\n",
        "        \n",
        "    train_features = feature_extractor.predict_proba(features_train)\n",
        "    test_features = feature_extractor.predict_proba(features_test)\n",
        "        \n",
        "    meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "\n",
        "    meta_model.fit(train_features, target_train)\n",
        "        \n",
        "    predictions = meta_model.predict(test_features)\n",
        "        \n",
        "    accuracy = accuracy_score(target_test, predictions)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
